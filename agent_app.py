import os
from datetime import datetime
from typing import List, Any

import ollama
import sys

from akshare_tools import *

# --- é…ç½® ---
DB_NAME = 'ollama_financial_agent.db'
MODEL_NAME = 'gpt-oss:20b'


# --- æ—¥å¿—é…ç½®ä¿®æ”¹ ---
LOG_DIR = 'debug'
LOG_FILENAME = datetime.now().strftime(f'{LOG_DIR}/%Y%m%d_%H%M%S.log')

if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
logging.basicConfig(level=logging.NOTSET, handlers=[])
formatter = logging.Formatter(
    '[%(asctime)s] - %(name)s - %(levelname)s - %(message)s'
)

file_handler = logging.FileHandler(LOG_FILENAME, encoding='utf-8')
file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶ä¸­è®°å½•æ‰€æœ‰ç»†èŠ‚
file_handler.setFormatter(formatter)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.FATAL)
console_handler.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

# è®¾ç½®æˆ‘ä»¬è‡ªå·±çš„ logger å®ä¾‹
logger = logging.getLogger(__name__)

logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚è¯¦ç»†æ—¥å¿—å·²å†™å…¥ï¼š{LOG_FILENAME}")

# è¿™ä¸ªç³»ç»Ÿæç¤ºå‘Šè¯‰ LLM å®ƒå¯ä»¥è°ƒç”¨å“ªäº›å‡½æ•°ä»¥åŠå®ƒä»¬çš„ JSON æè¿°
TOOL_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "get_stock_zh_a_spot_data",
            "description": "è·å– A è‚¡çš„å†å²è¡Œæƒ…æ•°æ®ï¼Œå¹¶å°†æ•°æ®å­˜å…¥ SQLite æ•°æ®åº“çš„å›ºå®šè¡¨ä¸­ã€‚è¿”å›åŒ…å«æœ€æ–°ä»·æ ¼çš„æ‘˜è¦ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbol": {"type": "string", "description": "è‚¡ç¥¨ä»£ç æˆ–æŒ‡æ•°ä»£ç ï¼Œå¦‚ '600519' æˆ– 'sh000001'"},
                    "period": {"type": "string", "description": "æ•°æ®å‘¨æœŸ ('daily', 'weekly', 'monthly')"},
                    "start_date": {"type": "string", "description": "å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'"},
                    "end_date": {"type": "string", "description": "ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'"}
                },
                "required": ["symbol"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "visualize_stock_data_trend",
            "description": "ä»æ•°æ®åº“ä¸­è¯»å–å›ºå®šè¡¨ä¸­çš„æ•°æ®ï¼Œè®¡ç®—æ»‘åŠ¨å¹³å‡çº¿ï¼Œç”Ÿæˆå¹¶æ˜¾ç¤ºä»·æ ¼è¶‹åŠ¿å›¾ã€‚ç”¨äºå“åº”'ç”»å›¾'ã€'èµ°åŠ¿å›¾'ç­‰è¯·æ±‚ã€‚æ³¨æ„ï¼šæ­¤å·¥å…·éœ€åœ¨'get_stock_zh_a_spot_data'è°ƒç”¨åä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbol": {"type": "string", "description": "è‚¡ç¥¨ä»£ç ï¼Œä»…ç”¨äºå›¾è¡¨æ ‡é¢˜ã€‚"}
                },
                "required": ["symbol"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "query_macro_data",
            "description": "æŸ¥è¯¢ä¸­å›½çš„å¹´åº¦å®è§‚ç»æµæ•°æ®ï¼Œå¦‚ CPI æˆ– GDPã€‚ç”¨äºå“åº”'æœ€æ–°CPI'ã€'GDPæ•°æ®'ç­‰è¯·æ±‚ã€‚æ•°æ®æ¥è‡ª AkShareã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "indicator": {
                        "type": "string",
                        "description": "è¦æŸ¥è¯¢çš„å®è§‚ç»æµæŒ‡æ ‡ï¼Œç›®å‰ä»…æ”¯æŒ 'CPI' (å¹´åº¦) æˆ– 'GDP' (å¹´åº¦)ã€‚",
                        "enum": ["CPI", "GDP"]
                    }
                },
                "required": ["indicator"]
            }
        }
    }
]

tool_schema_str = json.dumps(TOOL_SCHEMA, indent=2)

SYSTEM_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä¸»è¦ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·è·å–ã€å¤„ç†å’Œå¯è§†åŒ–è‚¡ç¥¨å’ŒæŒ‡æ•°æ•°æ®ã€‚

---
æ ¸å¿ƒæŒ‡ä»¤ï¼š
1. **ä»»åŠ¡ä¸å·¥å…·åŒ¹é…ï¼š** å¦‚æœç”¨æˆ·çš„è¯·æ±‚æ¶‰åŠåˆ°è·å–å¸‚åœºæ•°æ®ï¼ˆå¦‚ä»·æ ¼ã€å†å²æ•°æ®ã€è¶‹åŠ¿ã€èµ°åŠ¿å›¾ï¼‰ï¼Œä½ **å¿…é¡»**ä½¿ç”¨æä¾›çš„å·¥å…·ã€‚
2. **å·¥å…·è°ƒç”¨ï¼š** å¦‚æœä½ å†³å®šè°ƒç”¨å·¥å…·ï¼Œè¯·ç›´æ¥æŒ‰ç…§ Ollama/GPT-OSS è§„èŒƒè¿”å›ç»“æ„åŒ–çš„ `tool_calls` å­—æ®µã€‚
3. **æœ€ç»ˆå›å¤ï¼š** åœ¨å·¥å…·è°ƒç”¨å®Œæˆåï¼ŒåŸºäºå·¥å…·è¿”å›çš„ç»“æœè¿›è¡Œä¸“ä¸šçš„åˆ†æå’Œå›å¤ã€‚
4. **ç®€æ´æ€§ï¼š** é™¤éå¿…è¦ï¼Œé¿å…å†—é•¿æˆ–ä¸ç›¸å…³çš„è®¨è®ºã€‚

ä½ æ‹¥æœ‰ä»¥ä¸‹å·¥å…·ï¼ˆJSON Schemaï¼‰ï¼š

{}

---
"""

SYSTEM_PROMPT = SYSTEM_PROMPT_TEMPLATE.format(tool_schema_str)# æ‹¼åˆSYSTEM_PROMPT_TEMPLATEä¸TOOL_SCHEMA


def init_db():  # åˆå§‹åŒ–æ•°æ®åº“
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("""
                   CREATE TABLE IF NOT EXISTS conversation
                   (
                       id
                       INTEGER
                       PRIMARY
                       KEY
                       AUTOINCREMENT,
                       role
                       TEXT
                       NOT
                       NULL,
                       content
                       TEXT
                       NOT
                       NULL,
                       timestamp
                       DATETIME
                       DEFAULT
                       CURRENT_TIMESTAMP
                   )
                   """)
    conn.commit()
    conn.close()
    logger.info(f"SQLite æ•°æ®åº“ '{DB_NAME}' åˆå§‹åŒ–å®Œæˆ.")


def save_message(role, content):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO conversation (role, content) VALUES (?, ?)",
        (role, content)
    )
    conn.commit()
    conn.close()
    logger.info(f"æ¶ˆæ¯å·²ä¿å­˜: Role='{role}' | Content Length={len(content)}")


def load_context():
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute("SELECT role, content FROM conversation ORDER BY timestamp ASC")
    history_records = cursor.fetchall()
    conn.close()

    messages = []
    # å†å²è®°å½•ä¸­çš„ assistant æ¶ˆæ¯å¯èƒ½æ˜¯ Tool Call æˆ–æ™®é€šå“åº”ï¼Œéƒ½éœ€è¦ä¿æŒ
    for role, content in history_records:
        messages.append({'role': role, 'content': content})

    logger.info(f"å·²åŠ è½½ {len(messages)} æ¡å†å²æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡.")
    return messages


def clear_screen():
    os.system('cls')


# Tool Calling è§£æä¸æ‰§è¡Œ

def execute_tool_call(tool_calls: List[Any]) -> str:    # ç›´æ¥æ¥æ”¶æ¨¡å‹è¿”å›çš„ ToolCall å¯¹è±¡åˆ—è¡¨ï¼Œæ‰§è¡Œå·¥å…·ï¼Œå¹¶è¿”å› JSON å­—ç¬¦ä¸²ç»“æœã€‚
    if not tool_calls or not isinstance(tool_calls, list):
        return json.dumps({"error": "å·¥å…·æ‰§è¡Œå¤±è´¥: è¾“å…¥ä¸æ˜¯æœ‰æ•ˆçš„ ToolCall åˆ—è¡¨ã€‚"})

    # æå–ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼ˆå‡è®¾åªå¤„ç†ç¬¬ä¸€ä¸ªï¼‰
    try:
        tool_call = tool_calls[0]

        # æå–å·¥å…·åç§°å’Œå‚æ•°
        tool_name = tool_call.function.name
        tool_args = tool_call.function.arguments  # Dict[str, Any]

    except AttributeError:
        # å¤„ç†å±æ€§è®¿é—®é”™è¯¯ï¼Œå¦‚æœç»“æ„ä¸é¢„æœŸä¸ç¬¦
        # åœ¨æ›´æ¢ä¸º gpt-oss:20b åæœ‰å¾ˆå¤§æ”¹å–„
        return json.dumps({"error": "å·¥å…·æ‰§è¡Œå¤±è´¥: æ— æ³•ä» ToolCall å¯¹è±¡ä¸­è§£æå‡º function/name/arguments å±æ€§ã€‚"})
    except Exception as e:
        return json.dumps({"error": f"å·¥å…·æ‰§è¡Œå¤±è´¥: è§£æ ToolCall æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"})

    # 2. æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
    try:
        if tool_name not in AVAILABLE_TOOLS:
            return json.dumps({"error": f"æ‰¾ä¸åˆ°å·¥å…·: {tool_name}"})

        tool_function = AVAILABLE_TOOLS[tool_name]

        # æ‰§è¡Œå·¥å…·
        tool_result = tool_function(**tool_args)

        return tool_result  # tool_result å·²ç»æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œä¿æŒè¿”å›ç±»å‹ä¸€è‡´

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        import traceback
        traceback.print_exc()
        return json.dumps({"error": f"å·¥å…·å‡½æ•° '{tool_name}' æ‰§è¡Œå¤±è´¥: {e}"})


def chat_with_context(user_input): # ä¸»å¯¹è¯é€»è¾‘
    # 1. å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ (åŒ…å«ç³»ç»Ÿæç¤ºå’Œå†å²è®°å½•)
    context_messages = load_context()
    context_messages.insert(0, {'role': 'system', 'content': SYSTEM_PROMPT})
    context_messages.append({'role': 'user', 'content': user_input})

    # å°†æœ€æ–°çš„ç”¨æˆ·è¾“å…¥ä¿å­˜åˆ°æ•°æ®åº“
    save_message('user', user_input)

    # 2. åˆå§‹ Ollama è°ƒç”¨ (å¯èƒ½ä¼šè¿”å› Tool Call)
    print(f"\nğŸ‘¤ ä½ : {user_input}\n")
    print("ğŸ¤– Agent å¤„ç†ä¸­...")


    # LLM å“åº”çš„ç¬¬ä¸€éƒ¨åˆ†
    response = ollama.chat(
        model=MODEL_NAME,
        messages=context_messages,
        stream=False,
    )

    message = response['message']

    # æ ¸å¿ƒåˆ¤æ–­é€»è¾‘
    # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls å­—æ®µï¼Œä¸”å®ƒæ˜¯ä¸€ä¸ªéç©ºçš„åˆ—è¡¨
    if 'tool_calls' in message and isinstance(message['tool_calls'], list) and message['tool_calls']:

        tool_call_list = message['tool_calls']

        # æå–ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
        first_tool_call = tool_call_list[0]
        tool_function = first_tool_call['function']

        tool_name = tool_function['name']
        tool_args = tool_function['arguments']

        # æ„å»º JSON å­—ç¬¦ä¸²ç”¨äºæ•°æ®åº“å’Œ LLM çš„ç¬¬äºŒæ¬¡è°ƒç”¨
        tool_call_data = {"tool": tool_name, "arguments": tool_args}
        tool_call_json = json.dumps(tool_call_data)

        # 3. å·¥å…·æ‰§è¡Œé€»è¾‘
        print(f"Agent: **å·²è¯†åˆ«åˆ°å·¥å…·è°ƒç”¨**ï¼Œæ­£åœ¨æ‰§è¡Œ...")
        save_message('assistant', tool_call_json)
        logger.info(f"LLM è¯†åˆ«ä¸ºå·¥å…·è°ƒç”¨ï¼Œæ­£åœ¨æ‰§è¡Œï¼š{tool_call_json[:100]}...")

        tool_output = execute_tool_call(message.tool_calls)

        # 4. ç¬¬äºŒæ¬¡ Ollama è°ƒç”¨ (å¸¦ç€å·¥å…·ç»“æœ)
        logger.info("è¿›è¡Œç¬¬äºŒæ¬¡ LLM è°ƒç”¨ (å¸¦å·¥å…·ç»“æœ) ä»¥è·å–æœ€ç»ˆå›å¤...")

        # 4a. å‡†å¤‡ç¬¬äºŒæ¬¡è°ƒç”¨çš„æ¶ˆæ¯åˆ—è¡¨ (ä¿æŒä¸å˜)
        second_call_messages = [{'role': 'system', 'content': SYSTEM_PROMPT}] + context_messages.copy()
        second_call_messages.append({'role': 'user', 'content': user_input})
        second_call_messages.append({'role': 'assistant', 'content': tool_call_json})
        second_call_messages.append({'role': 'tool', 'content': tool_output})

        # 4b. ç¬¬äºŒæ¬¡æµå¼è°ƒç”¨
        final_response = ollama.chat(
            model=MODEL_NAME,
            messages=second_call_messages,
            stream=False
        )

        # 4c. æå–æœ€ç»ˆå“åº”
        final_answer = final_response.message.content.strip()

        print("\n   Agent æœ€ç»ˆå›å¤: ")
        print(final_answer)  # ç›´æ¥æ‰“å°å®Œæ•´å›å¤

        print("\n" + "-" * 30 + "\n")
    else:

        # æå– content å­—æ®µä½œä¸ºæœ€ç»ˆå›å¤
        full_response_content = message.get('content', '').strip()

        if not full_response_content:
            # å¦‚æœæ¨¡å‹æ²¡æœ‰è¿”å› contentï¼Œä½†ä¹Ÿæ²¡æœ‰ tool_callsï¼Œå¯èƒ½æ˜¯é—²èŠæ¨¡å¼çš„æ€è€ƒè¿‡ç¨‹
            full_response_content = "æˆ‘æ²¡æœ‰æ‰¾åˆ°å¯ä»¥æ‰§è¡Œçš„å·¥å…·ï¼Œè¯·æ˜ç¡®æ‚¨çš„é—®é¢˜ã€‚"

        # 3. ç›´æ¥å›å¤
        print(" Agent æœ€ç»ˆå›å¤: ")
        print(full_response_content)
        print("\n" + "-" * 30 + "\n")

        # 4. ä¿å­˜ç¬¬ä¸€ä¸ªå“åº”
        save_message('assistant', full_response_content)

# ä¸»å¾ªç¯
def interactive_chat():
    init_db()

    while True:
        try:
            user_input = input("è¯·è¾“å…¥ä½ çš„é‡‘èé—®é¢˜ (è¾“å…¥ 'exit' é€€å‡º, 'clear' æ¸…ç©ºå†å²): \n> ")
        except EOFError:
            break

        if user_input.lower() == 'exit':
            break

        if user_input.lower() == 'clear':
            conn = sqlite3.connect(DB_NAME)
            conn.execute("DELETE FROM conversation")
            conn.commit()
            conn.close()
            logger.info("å¯¹è¯å†å²å·²æ¸…ç©ºï¼")
            continue

        if user_input.strip():
            chat_with_context(user_input)

    logger.info("é‡‘è AI Agent å·²é€€å‡ºã€‚")


if __name__ == '__main__':
    clear_screen()
    interactive_chat()
    # åŠ è½½å¹¶æ˜¾ç¤ºå†å²æ¦‚è¦
    history = load_context()
    if history:
        print(f"--- å†å²è®°å½• ({len(history)} æ¡æ¶ˆæ¯) ---")
        last_user_msg = next((m['content'] for m in reversed(history) if m['role'] == 'user'), "æ— ")
        print(f"æœ€è¿‘ç”¨æˆ·: {last_user_msg[:50]}...")
        print("-" * 30)
    else:
        print("--- å¼€å§‹æ–°å¯¹è¯ ---")