import logging
import sqlite3
import json
import uuid
from abc import ABC, abstractmethod
from typing import Dict, Any

import ollama

from agent_app import save_message, load_context, SYSTEM_PROMPT
from akshare_tools import AVAILABLE_TOOLS
from utils.context_store import ContextStore

MODEL_NAME = 'gpt-oss:20b'
LOG_DIR = 'debug'
logging.basicConfig(
        level=logging.INFO,
        format=(
            "%(asctime)s | %(levelname)s | "
            "task=%(task_id)s | agent=%(agent)s | "
            "%(message)s"
        ),
        datefmt="%Y-%m-%d %H:%M:%S",)


class ChatHistoryDB:
    def __init__(self, db_name="agent_memory.db"):
        self.conn = sqlite3.connect(db_name)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS chat_logs 
            (id INTEGER PRIMARY KEY AUTOINCREMENT, 
             agent_name TEXT, 
             input_text TEXT, 
             output_text TEXT, 
             timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)
        ''')
        self.conn.commit()

    def save_log(self, agent_name, input_data, output_data):
        self.cursor.execute(
            "INSERT INTO chat_logs (agent_name, input_text, output_text) VALUES (?, ?, ?)",
            (agent_name, str(input_data), str(output_data))
        )
        self.conn.commit()


class AgentContext:
    def __init__(self, user_input: str):
        self.user_input = user_input
        self.messages = []
        self.iteration = 0
        self.finished = False

class Agent(ABC):
    def __init__(self, name: str, store: ContextStore):
        self.name = name
        self.store = store
        self.logger = None

    def _bind_logger(self, task_id: str):
        base_logger = logging.getLogger(self.name)
        self.logger = logging.LoggerAdapter(
            base_logger,
            {
                "task_id": task_id,
                "agent": self.name,
            }
        )

    @abstractmethod
    def run(self, ctx: dict) -> None:
        pass

    def persist(self, ctx: dict):
        self.store.save_context(ctx["task_id"], ctx, self.name)
        self.logger.info("context persisted to sqlite")

class DataFetchAgent(Agent):
    def run(self, ctx: dict) -> None:
        print("ğŸ“¡ DataFetchAgent")

        ctx["raw_data"] = {
            "source": "demo",
            "values": [1, 2, 3, 4, 5]
        }

        self.persist(ctx)

def execute_single_tool(tool_name: str, tool_args: Dict[str, Any]) -> str:
    """
    æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨ã€‚
    æ¥æ”¶å‡½æ•°åå’Œå‚æ•°å­—å…¸ï¼Œè¿”å› JSON å­—ç¬¦ä¸²ç»“æœã€‚
    """

    # 1. æŸ¥æ‰¾å·¥å…·
    if tool_name not in AVAILABLE_TOOLS:
        error_msg = f"æ‰§è¡Œå¤±è´¥: æ‰¾ä¸åˆ°å·¥å…· '{tool_name}'"
        print(f"âŒ {error_msg}")
        return json.dumps({"error": error_msg}, ensure_ascii=False)

    tool_function = AVAILABLE_TOOLS[tool_name]

    try:
        # 2. æ‰§è¡Œå·¥å…·
        # ä½¿ç”¨ ** è§£åŒ…å­—å…¸å‚æ•°ä¼ å…¥å‡½æ•°
        print(f"âš™ï¸ æ­£åœ¨è°ƒç”¨: {tool_name}({tool_args})")
        result = tool_function(**tool_args)

        # 3. ç»Ÿä¸€è¿”å›å€¼æ ¼å¼
        # LLM éœ€è¦æ¥æ”¶ String ç±»å‹çš„ contentã€‚
        # å¦‚æœå·¥å…·è¿”å›çš„æ˜¯å­—å…¸ã€åˆ—è¡¨ç­‰å¯¹è±¡ï¼Œå¿…é¡»è½¬ä¸º JSON å­—ç¬¦ä¸²ã€‚
        if isinstance(result, (dict, list, int, float, bool)):
            return json.dumps(result, ensure_ascii=False)

        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ JSON å­—ç¬¦ä¸²ï¼‰ï¼Œç›´æ¥è¿”å›
        return str(result)

    except TypeError as e:
        # æ•æ‰å‚æ•°é”™è¯¯ï¼ˆä¾‹å¦‚æ¨¡å‹å¹»è§‰ç”Ÿæˆäº†ä¸å­˜åœ¨çš„å‚æ•°ï¼‰
        error_msg = f"å‚æ•°é”™è¯¯: å·¥å…· '{tool_name}' ä¸æ¥å—æä¾›çš„å‚æ•°: {e}"
        print(f"âŒ {error_msg}")
        return json.dumps({"error": error_msg}, ensure_ascii=False)

    except Exception as e:
        # æ•æ‰å·¥å…·å†…éƒ¨è¿è¡Œæ—¶çš„å…¶ä»–å¼‚å¸¸# æ‰“å°å †æ ˆä¿¡æ¯æ–¹ä¾¿è°ƒè¯•
        error_msg = f"è¿è¡Œæ—¶é”™è¯¯: å·¥å…· '{tool_name}' æ‰§è¡Œå¼‚å¸¸: {str(e)}"
        return json.dumps({"error": error_msg}, ensure_ascii=False)


TOOL_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "get_stock_zh_a_spot_data",
            "description": "è·å– A è‚¡çš„å†å²è¡Œæƒ…æ•°æ®ï¼Œå¹¶å°†æ•°æ®å­˜å…¥ SQLite æ•°æ®åº“çš„å›ºå®šè¡¨ä¸­ã€‚è¿”å›åŒ…å«æœ€æ–°ä»·æ ¼çš„æ‘˜è¦ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbol": {"type": "string"},
                    "period": {"type": "string"},
                    "start_date": {"type": "string"},
                    "end_date": {"type": "string"}
                },
                "required": ["symbol"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "visualize_stock_data_trend",
            "description": "ä»æ•°æ®åº“è¯»å–æ•°æ®å¹¶ç”Ÿæˆèµ°åŠ¿å›¾ï¼ˆéœ€åœ¨æ•°æ®è·å–åè°ƒç”¨ï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbol": {"type": "string"}
                },
                "required": ["symbol"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "query_macro_data",
            "description": "æŸ¥è¯¢ä¸­å›½å¹´åº¦å®è§‚ç»æµæ•°æ®ï¼ˆCPI / GDPï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "indicator": {"type": "string", "enum": ["CPI", "GDP"]}
                },
                "required": ["indicator"]
            }
        }
    }
]

TOOL_SCHEMA_STR = json.dumps(TOOL_SCHEMA, indent=2, ensure_ascii=False)
DATA_FETCH_PROMPT = f"""
ä½ æ˜¯ã€æ•°æ®è·å– Agentã€‘ã€‚

èŒè´£ï¼š
- åˆ¤æ–­æ˜¯å¦éœ€è¦çœŸå®é‡‘è/å®è§‚æ•°æ®
- å¿…é¡»é€šè¿‡å·¥å…·è·å–
- æ•°æ®ä¼šè‡ªåŠ¨å­˜å…¥ SQLite

ğŸš¨ è§„åˆ™ï¼š
1. ä¸¥ç¦çŒœæµ‹ä»»ä½•æ•°å€¼
2. æ¶‰åŠè¡Œæƒ… / CPI / GDP â†’ å¿…é¡»è°ƒç”¨å·¥å…·
3. åªå…è®¸ä½¿ç”¨ï¼š
   - get_stock_zh_a_spot_data
   - query_macro_data
4. ä¸å¾—è¿›è¡Œåˆ†ææˆ–æ€»ç»“

å¯ç”¨å·¥å…·ï¼š
{TOOL_SCHEMA_STR}
"""

DATA_PROCESS_PROMPT = """
ä½ æ˜¯ã€æ•°æ®å¤„ç† Agentã€‘ã€‚

èŒè´£ï¼š
- åŸºäºå·²è·å–å¹¶å­˜å…¥ SQLite çš„æ•°æ®è¿›è¡Œåˆ†æ
- åœ¨ç”¨æˆ·è¯·æ±‚â€œç”»å›¾ / èµ°åŠ¿å›¾â€æ—¶ç”Ÿæˆå›¾è¡¨

ğŸš¨ è§„åˆ™ï¼š
1. ç¦æ­¢é‡æ–°è·å–æ•°æ®
2. åªå…è®¸ä½¿ç”¨ visualize_stock_data_trend
3. è‹¥ç¼ºå°‘å‰ç½®æ•°æ®ï¼Œå¿…é¡»è¯´æ˜

ä¸å†™æœ€ç»ˆæŠ¥å‘Š
"""

REPORT_PROMPT = """
ä½ æ˜¯ã€æŠ¥å‘Šæ’°å†™ Agentã€‘ã€‚

èŒè´£ï¼š
- å°†å·²æœ‰åˆ†æç»“æœæ•´ç†ä¸ºæœ€ç»ˆæŠ¥å‘Š

ğŸš¨ è§„åˆ™ï¼š
1. ä¸¥ç¦è°ƒç”¨ä»»ä½•å·¥å…·
2. ä¸è¡¥å……ä»»ä½•æœªç»™å‡ºçš„æ•°æ®
3. ä¸æ Agent / å·¥å…· / æ•°æ®åº“

ç›´æ¥è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
"""

class DataFetchAgent(Agent):
    def run(self, ctx: dict):
        self.bind_logger(ctx["task_id"])
        self.logger.info("start")

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": ctx["user_input"]},
        ]

        resp = ollama.chat(
            model=ctx["model"],
            messages=messages,
            tools=TOOL_SCHEMA,
        )

        msg = resp["message"]
        ctx["messages"].append(msg)

        if msg.get("tool_calls"):
            for tool in msg["tool_calls"]:
                result = execute_single_tool(
                    tool["function"]["name"],
                    tool["function"]["arguments"],
                )
                ctx["messages"].append({
                    "role": "tool",
                    "content": str(result),
                })

        self.store.save(ctx["task_id"], ctx)
        self.logger.info("finished")


class DataProcessAgent(Agent):
    def run(self, ctx: dict):
        self.bind_logger(ctx["task_id"])
        self.logger.info("start")

        messages = [
            {"role": "system", "content": self.system_prompt},
            *ctx["messages"],
        ]

        resp = ollama.chat(
            model=ctx["model"],
            messages=messages,
            tools=TOOL_SCHEMA,
        )

        msg = resp["message"]
        ctx["messages"].append(msg)

        if msg.get("tool_calls"):
            for tool in msg["tool_calls"]:
                result = execute_single_tool(
                    tool["function"]["name"],
                    tool["function"]["arguments"],
                )
                ctx["messages"].append({
                    "role": "tool",
                    "content": str(result),
                })

        self.store.save(ctx["task_id"], ctx)
        self.logger.info("finished")


class ReportAgent(Agent):
    def run(self, ctx: dict):
        self.bind_logger(ctx["task_id"])
        self.logger.info("start")

        messages = [
            {"role": "system", "content": self.system_prompt},
            *ctx["messages"],
        ]

        resp = ollama.chat(
            model=ctx["model"],
            messages=messages,
            stream=False,
        )

        ctx["final_report"] = resp["message"]["content"]
        self.store.save(ctx["task_id"], ctx)

        self.logger.info("finished")


def run(user_input: str):

    store = ContextStore()
    task_id = str(uuid.uuid4())

    ctx = {
        "task_id": task_id,
        "user_input": user_input,
        "model": "qwen2.5:14b",
        "messages": [],
        "final_report": None,
    }

    store.save(task_id, ctx)

    agents = [
        DataFetchAgent("DataFetchAgent", DATA_FETCH_PROMPT, store),
        DataProcessAgent("DataProcessAgent", DATA_PROCESS_PROMPT, store),
        ReportAgent("ReportAgent", REPORT_PROMPT, store),
    ]

    for agent in agents:
        agent.run(ctx)

    print("\n===== æœ€ç»ˆæŠ¥å‘Š =====\n")
    print(ctx["final_report"])